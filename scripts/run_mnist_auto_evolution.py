#!/usr/bin/env python3
"""
MNIST è‡ªåŠ¨æ¼”åŒ–å®éªŒè„šæœ¬
æµ‹è¯• vision_pipeline é€šè¿‡è‡ªåŠ¨æ¼”åŒ–åˆ›å»ºï¼ˆè€Œéç¡¬ç¼–ç ï¼‰
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

import json
import argparse
from typing import Dict, Optional

import torch
from torch.optim import Adam
from tqdm import tqdm

from aonn.models.mnist_world_model import MNISTWorldModel, MNISTWorldInterface
from aonn.models.aonn_brain_v3 import AONNBrainV3
from aonn.core.active_inference_loop import ActiveInferenceLoop
from aonn.aspects.classification_aspect import ClassificationAspect


def run_experiment(
    num_steps: int,
    config: Dict,
    device: torch.device,
    *,
    verbose: bool = False,
    output: str = "data/mnist_auto_evolution.json",
    save_interval: int = 100,
):
    """è¿è¡Œ MNIST è‡ªåŠ¨æ¼”åŒ–å®éªŒ"""
    
    # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯ä¸–ç•Œæ¨¡å‹
    train_world = MNISTWorldModel(
        state_dim=config.get("state_dim", 128),
        action_dim=config.get("act_dim", 10),
        obs_dim=config.get("obs_dim", 784),
        device=device,
        train=True,
    )
    train_interface = MNISTWorldInterface(train_world)
    
    val_world = MNISTWorldModel(
        state_dim=config.get("state_dim", 128),
        action_dim=config.get("act_dim", 10),
        obs_dim=config.get("obs_dim", 784),
        device=device,
        train=False,  # ä½¿ç”¨æµ‹è¯•é›†
    )
    val_interface = MNISTWorldInterface(val_world)
    
    # åˆ›å»º AONN Brainï¼ˆæœ€å°åˆå§‹æ¶æ„ï¼Œä¸åŒ…å« vision_pipelineï¼‰
    brain = AONNBrainV3(config=config, device=device, enable_evolution=True)
    
    # åˆ›å»º target Object
    brain.create_object("target", dim=10)
    
    print("åˆå§‹åŒ– AONN Brainï¼ˆè‡ªåŠ¨æ¼”åŒ–æ¨¡å¼ï¼‰...")
    print("  âœ“ åˆå§‹ç½‘ç»œï¼šæœ€å°æ¶æ„ï¼ˆæ—  vision_pipelineï¼‰")
    print("  âœ“ ç­‰å¾…è‡ªåŠ¨æ¼”åŒ–åˆ›å»º vision_pipeline...")
    print()
    
    # åˆ›å»ºåˆ†ç±»å™¨ï¼ˆè¿™ä¸ªå¯ä»¥é¢„å…ˆåˆ›å»ºï¼Œå› ä¸ºåˆ†ç±»å™¨ä¸æ˜¯è‡ªåŠ¨æ¼”åŒ–çš„ç›®æ ‡ï¼‰
    classification_aspect = brain.create_unified_aspect(
        aspect_type="classification",
        src_names=["internal"],
        dst_names=["target"],
        name="mnist_classifier",
        state_dim=config.get("state_dim", 128),
        num_classes=10,
        hidden_dim=config.get("state_dim", 128),
        loss_weight=config.get("classification_loss_weight", 1.0),
    )
    print(f"  âœ“ åˆ›å»º mnist_classifier (state_dim={config.get('state_dim', 128)}, num_classes=10)")
    print()
    
    # åˆ›å»º Adam ä¼˜åŒ–å™¨ï¼ˆåˆå§‹åªåŒ…å«åˆ†ç±»å™¨ï¼Œvision_pipeline ä¼šåœ¨æ¼”åŒ–åæ·»åŠ ï¼‰
    aspect_params = list(classification_aspect.parameters())
    aspect_optimizer = Adam(
        aspect_params,
        lr=config.get("learning_rate", 0.001),
        betas=(0.9, 0.999),
        eps=1e-8,
    )
    print(f"  âœ“ åˆå§‹åŒ– Adam ä¼˜åŒ–å™¨: lr={config.get('learning_rate', 0.001)}")
    print()
    
    snapshots = []
    accuracy_history = []
    free_energy_history = []
    evolution_events = []  # è®°å½•æ¼”åŒ–äº‹ä»¶
    
    # åˆå§‹åŒ–è§‚å¯Ÿ
    obs = train_interface.reset()
    action = None
    prev_obs = None
    
    # æ£€æŸ¥ vision_pipeline æ˜¯å¦å·²åˆ›å»º
    vision_pipeline_created = False
    
    progress = tqdm(range(num_steps), desc=f"MNIST Auto Evolution {num_steps}")
    
    try:
        for step in progress:
            # 1. è·å–è§‚å¯Ÿå’Œç›®æ ‡
            if step > 0:
                obs, reward = train_interface.step(action)
            else:
                obs = train_interface.reset()
            
            # è®¾ç½®è§‚å¯Ÿ
            for sense, value in obs.items():
                if sense in brain.objects:
                    brain.objects[sense].set_state(value)
            
            # è·å–ç›®æ ‡æ ‡ç­¾
            target = train_interface.get_target()
            brain.objects["target"].set_state(target)
            
            # 2. ç½‘ç»œæ¼”åŒ–ï¼ˆè¿™é‡Œä¼šè‡ªåŠ¨åˆ›å»º vision_pipelineï¼‰
            num_aspects_before = len(brain.aspects)
            brain.evolve_network(obs)
            num_aspects_after = len(brain.aspects)
            
            # æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº† vision_pipeline
            if not vision_pipeline_created:
                vision_pipeline = None
                for asp in brain.aspects:
                    if (hasattr(asp, 'name') and 
                        ('vision' in asp.name.lower() and 'pipeline' in asp.name.lower())):
                        vision_pipeline = asp
                        vision_pipeline_created = True
                        evolution_events.append({
                            "step": step + 1,
                            "event": "vision_pipeline_created",
                            "name": asp.name,
                            "free_energy": brain.compute_free_energy().item(),
                        })
                        print(f"\nğŸ‰ [Step {step+1}] è‡ªåŠ¨æ¼”åŒ–åˆ›å»ºäº† vision_pipeline: {asp.name}")
                        # æ›´æ–°ä¼˜åŒ–å™¨ï¼Œæ·»åŠ  vision_pipeline çš„å‚æ•°
                        aspect_params = list(asp.parameters()) + list(classification_aspect.parameters())
                        aspect_optimizer = Adam(
                            aspect_params,
                            lr=config.get("learning_rate", 0.001),
                            betas=(0.9, 0.999),
                            eps=1e-8,
                        )
                        print(f"  âœ“ æ›´æ–°ä¼˜åŒ–å™¨ï¼Œæ·»åŠ  vision_pipeline å‚æ•°")
                        break
            
            # å¦‚æœ vision_pipeline å·²åˆ›å»ºï¼Œè·å–å®ƒ
            if vision_pipeline_created and vision_pipeline is None:
                for asp in brain.aspects:
                    if (hasattr(asp, 'name') and 
                        ('vision' in asp.name.lower() and 'pipeline' in asp.name.lower())):
                        vision_pipeline = asp
                        break
            
            # 3. ä¸»åŠ¨æ¨ç†ï¼ˆçŠ¶æ€æ¨ç†ï¼‰
            if len(brain.aspects) > 0:
                try:
                    # ç¡®ä¿çŠ¶æ€æ˜¯ detached çš„
                    for obj_name, obj in brain.objects.items():
                        state = obj.state
                        if state.requires_grad and state.is_leaf and state.grad is not None:
                            obj.set_state(state.detach())
                        elif not state.is_leaf:
                            obj.set_state(state.detach())
                    
                    loop = ActiveInferenceLoop(
                        brain.objects,
                        brain.aspects,
                        infer_lr=config.get("infer_lr", 0.01),
                        max_grad_norm=config.get("max_grad_norm", 100.0),
                        device=device,
                    )
                    num_iters = config.get("num_infer_iters", 5)
                    loop.infer_states(
                        target_objects=("internal",),
                        num_iters=num_iters,
                        sanitize_callback=brain.sanitize_states
                    )
                    brain.sanitize_states()
                except Exception as e:
                    if verbose:
                        print(f"Step {step}: Inference error: {e}")
                    pass
            
            # 4. å‚æ•°å­¦ä¹ ï¼ˆä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼‰
            if step > 0 and vision_pipeline_created:  # åªæœ‰ vision_pipeline åˆ›å»ºåæ‰å­¦ä¹ 
                try:
                    aspect_optimizer.zero_grad()
                    F = brain.compute_free_energy()
                    
                    if torch.isfinite(F) and F.requires_grad:
                        F.backward()
                        
                        # æ¢¯åº¦è£å‰ª
                        max_grad_norm = config.get("max_grad_norm", None)
                        if max_grad_norm is not None:
                            torch.nn.utils.clip_grad_norm_(aspect_params, max_grad_norm)
                        
                        aspect_optimizer.step()
                        brain.sanitize_states()
                except Exception as e:
                    if verbose:
                        print(f"Step {step}: Learning error: {e}")
                    pass
            
            # 5. ç”ŸæˆåŠ¨ä½œï¼ˆåˆ†ç±»é¢„æµ‹ï¼‰
            with torch.no_grad():
                logits = classification_aspect.predict(brain.objects)
                action = torch.softmax(logits, dim=-1)
                
                # è¯„ä¼°å‡†ç¡®ç‡
                pred_label = logits.argmax().item()
                true_label = train_interface.world_model.get_label()
                correct = (pred_label == true_label)
                accuracy_history.append(1.0 if correct else 0.0)
            
            # 6. è®°å½•è‡ªç”±èƒ½
            with torch.no_grad():
                F = brain.compute_free_energy()
                free_energy_history.append(F.item())
            
            # 7. ä¿å­˜å¿«ç…§
            if (step + 1) % save_interval == 0 or step == num_steps - 1:
                structure = brain.get_network_structure()
                avg_acc = sum(accuracy_history[-100:]) / min(100, len(accuracy_history))
                avg_F = sum(free_energy_history[-100:]) / min(100, len(free_energy_history))
                
                snapshot = {
                    "step": step + 1,
                    "free_energy": avg_F,
                    "accuracy": avg_acc,
                    "structure": structure,
                    "vision_pipeline_created": vision_pipeline_created,
                }
                snapshots.append(snapshot)
            
            # 8. æ›´æ–°è¿›åº¦æ¡
            avg_acc = sum(accuracy_history[-100:]) / min(100, len(accuracy_history))
            avg_F = sum(free_energy_history[-100:]) / min(100, len(free_energy_history))
            structure = brain.get_network_structure()
            
            vision_status = "âœ“" if vision_pipeline_created else "âœ—"
            progress.set_postfix(
                F=f"{avg_F:.3f}",
                Acc=f"{avg_acc*100:.1f}%",
                Asp=structure.get('num_aspects', 0),
                Pipe=structure.get('num_pipelines', 0),
                Vision=vision_status,
            )
            
            if verbose and (step + 1) % 50 == 0:
                print(f"[Step {step+1}] F={avg_F:.4f}, Acc={avg_acc*100:.2f}%, "
                      f"Aspects={structure.get('num_aspects', 0)}, "
                      f"Vision Pipeline={'Created' if vision_pipeline_created else 'Not Created'}")
            
            prev_obs = {sense: value.clone() for sense, value in obs.items()}
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ å®éªŒå‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    # éªŒè¯
    print("\nå¼€å§‹éªŒè¯...")
    val_accuracy = validate_with_world_model(
        brain=brain,
        val_interface=val_interface,
        classification_aspect=classification_aspect,
        num_samples=min(1000, len(val_world.dataset)),
        config=config,
        device=device,
    )
    
    # æœ€ç»ˆç»“æœ
    final_snapshot = brain.observe_self_model()
    final_F = free_energy_history[-1] if free_energy_history else 0.0
    final_accuracy = sum(accuracy_history[-100:]) / min(100, len(accuracy_history)) if accuracy_history else 0.0
    
    result = {
        "num_steps": num_steps,
        "vision_pipeline_created": vision_pipeline_created,
        "evolution_events": evolution_events,
        "final_free_energy": final_F,
        "final_train_accuracy": final_accuracy,
        "final_val_accuracy": val_accuracy,
        "final_structure": final_snapshot.get("structure", {}),
        "snapshots": snapshots,
        "accuracy_history": accuracy_history[-1000:],
        "free_energy_history": free_energy_history[-1000:],
        "evolution_summary": brain.evolution.get_evolution_summary() if brain.evolution else {},
    }
    
    return result


def validate_with_world_model(
    brain,
    val_interface,
    classification_aspect,
    num_samples: int,
    config: Dict,
    device: torch.device,
):
    """ä½¿ç”¨ä¸–ç•Œæ¨¡å‹éªŒè¯"""
    correct = 0
    
    with torch.no_grad():
        obs = val_interface.reset()
        
        for i in range(num_samples):
            # è®¾ç½®è§‚å¯Ÿ
            for sense, value in obs.items():
                if sense in brain.objects:
                    brain.objects[sense].set_state(value)
            
            # è·å–ç›®æ ‡ï¼ˆç”¨äºæ¨ç†ï¼‰
            target = val_interface.get_target()
            brain.objects["target"].set_state(target)
            
            # ä¸»åŠ¨æ¨ç†ï¼ˆä¸æ›´æ–°å‚æ•°ï¼‰
            if len(brain.aspects) > 0:
                try:
                    loop = ActiveInferenceLoop(
                        brain.objects,
                        brain.aspects,
                        infer_lr=config.get("infer_lr", 0.01),
                        max_grad_norm=config.get("max_grad_norm", 100.0),
                        device=device,
                    )
                    loop.infer_states(
                        target_objects=("internal",),
                        num_iters=config.get("num_infer_iters", 3),
                        sanitize_callback=brain.sanitize_states
                    )
                except Exception:
                    pass
            
            # é¢„æµ‹
            logits = classification_aspect.predict(brain.objects)
            pred_label = logits.argmax().item()
            true_label = val_interface.world_model.get_label()
            
            if pred_label == true_label:
                correct += 1
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ ·æœ¬
            action = torch.softmax(logits, dim=-1)
            obs, _ = val_interface.step(action)
    
    accuracy = correct / num_samples
    return accuracy


def main():
    parser = argparse.ArgumentParser(description="MNIST è‡ªåŠ¨æ¼”åŒ–å®éªŒ")
    parser.add_argument("--steps", type=int, default=500, help="è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--state-dim", type=int, default=128, help="çŠ¶æ€ç»´åº¦")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--output", type=str, default="data/mnist_auto_evolution.json", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", type=str, default="cpu", help="è®¾å¤‡")
    parser.add_argument("--save-interval", type=int, default=50, help="å¿«ç…§ä¿å­˜é—´éš”")
    
    args = parser.parse_args()
    
    device = torch.device(args.device)
    
    # é…ç½®ï¼ˆå¯ç”¨è‡ªåŠ¨æ¼”åŒ–åˆ›å»º vision_pipelineï¼‰
    config = {
        "obs_dim": 784,
        "state_dim": args.state_dim,
        "act_dim": 10,
        "sense_dims": {"vision": 784},
        "enable_world_model_learning": False,
        "evolution": {
            "free_energy_threshold": 2.0,  # è‡ªç”±èƒ½é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼ä¼šè§¦å‘è‡ªåŠ¨åˆ›å»º
            "prune_threshold": 0.01,
            "max_objects": 20,
            "max_aspects": 500,
            "error_ema_alpha": 0.5,
            "batch_growth": {
                "base": 0,
                "max_per_step": 0,
                "max_total": 0,
                "min_per_sense": 0,
            },
            "pipeline_growth": {
                "enable": False,  # ç¦ç”¨è‡ªåŠ¨åˆ›å»º internal->internal pipeline
            },
        },
        # å…³é”®é…ç½®ï¼šå¯ç”¨è‡ªåŠ¨åˆ›å»º vision_pipeline
        "pipeline_growth": {
            "use_pipeline_for_encoder": True,  # ä½¿ç”¨ Pipeline è€Œä¸æ˜¯ç®€å•çš„ Encoder
            "initial_width": 32,  # Pipeline å®½åº¦
            "initial_depth": 4,   # Pipeline æ·±åº¦
        },
        "infer_lr": 0.01,
        "learning_rate": 0.001,
        "classification_loss_weight": 1.0,
        "num_infer_iters": 5,
        "max_grad_norm": 100.0,
        "state_clip_value": 5.0,
    }
    
    print("=" * 80)
    print("MNIST è‡ªåŠ¨æ¼”åŒ–å®éªŒ")
    print("=" * 80)
    print(f"è®­ç»ƒæ­¥æ•°: {args.steps}")
    print(f"çŠ¶æ€ç»´åº¦: {config['state_dim']}")
    print(f"è§‚å¯Ÿç»´åº¦: {config['obs_dim']}")
    print(f"åŠ¨ä½œç»´åº¦: {config['act_dim']}")
    print(f"è‡ªç”±èƒ½é˜ˆå€¼: {config['evolution']['free_energy_threshold']}")
    print(f"è‡ªåŠ¨åˆ›å»º vision_pipeline: å¯ç”¨")
    print(f"Pipeline é…ç½®: depth={config['pipeline_growth']['initial_depth']}, "
          f"width={config['pipeline_growth']['initial_width']}")
    print("=" * 80)
    print()
    
    result = run_experiment(
        num_steps=args.steps,
        config=config,
        device=device,
        verbose=args.verbose,
        output=args.output,
        save_interval=args.save_interval,
    )
    
    # ä¿å­˜ç»“æœ
    output_path = Path(__file__).parent.parent / args.output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w") as f:
        json.dump(result, f, indent=2, default=str)
    
    print()
    print("=" * 80)
    print("å®éªŒå®Œæˆï¼")
    print("=" * 80)
    print(f"ç»“æœä¿å­˜åˆ°: {output_path}")
    print(f"vision_pipeline æ˜¯å¦è‡ªåŠ¨åˆ›å»º: {'æ˜¯' if result['vision_pipeline_created'] else 'å¦'}")
    if result['evolution_events']:
        for event in result['evolution_events']:
            print(f"  æ­¥éª¤ {event['step']}: {event['event']} ({event['name']}), "
                  f"è‡ªç”±èƒ½={event['free_energy']:.4f}")
    print(f"æœ€ç»ˆè‡ªç”±èƒ½: {result['final_free_energy']:.4f}")
    print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {result['final_train_accuracy']*100:.2f}%")
    print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {result['final_val_accuracy']*100:.2f}%")
    print(f"æœ€ç»ˆç»“æ„: {result['final_structure'].get('num_objects', 0)} Objects, "
          f"{result['final_structure'].get('num_aspects', 0)} Aspects, "
          f"{result['final_structure'].get('num_pipelines', 0)} Pipelines")
    print("=" * 80)


if __name__ == "__main__":
    main()

