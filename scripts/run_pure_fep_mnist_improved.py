#!/usr/bin/env python3
"""
çº¯ FEP MNIST å®éªŒï¼ˆæ”¹è¿›ç‰ˆï¼‰ï¼šè§£å†³ç¼–ç å™¨å’Œè§£ç å™¨æ”¶æ•›é—®é¢˜

æ ¸å¿ƒæ”¹è¿›ï¼š
1. æ–¹æ¡ˆ2ï¼šä½¿ç”¨ç¼–ç å™¨è¾“å‡ºä½œä¸º internal çš„åˆå§‹å€¼ï¼Œå‡å°‘çŠ¶æ€æ¨ç†è¿­ä»£æ¬¡æ•°
2. æ–¹æ¡ˆ5ï¼šå¯¹äºé™æ€åˆ†ç±»ä»»åŠ¡ï¼Œæä¾›é€‰é¡¹ç›´æ¥ä½¿ç”¨ç¼–ç å™¨è¾“å‡ºï¼ˆè·³è¿‡çŠ¶æ€æ¨ç†ï¼‰
3. è°ƒæ•´è‡ªç”±èƒ½æƒé‡ï¼Œå¹³è¡¡ç¼–ç å™¨å’Œè§£ç å™¨çš„å­¦ä¹ 
4. ä½¿ç”¨åˆ†ç¦»ä¼˜åŒ–å™¨ï¼Œä¸ºä¸åŒç»„ä»¶è®¾ç½®ä¸åŒå­¦ä¹ ç‡
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

import json
import argparse
from typing import Dict
import torch
import torch.nn as nn
from torch.optim import Adam
from tqdm import tqdm

from aonn.models.mnist_world_model import MNISTWorldModel, MNISTWorldInterface
from aonn.aspects.encoder_aspect import EncoderAspect
from aonn.aspects.world_model_aspects import ObservationAspect, DynamicsAspect, PreferenceAspect
from aonn.core.active_inference_loop import ActiveInferenceLoop
from aonn.core.object import ObjectNode
from aonn.core.free_energy import compute_total_free_energy


class PureFEPMNISTClassifier:
    """
    çº¯ FEP MNIST åˆ†ç±»å™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    """
    
    def __init__(
        self,
        state_dim: int = 128,
        obs_dim: int = 784,
        action_dim: int = 10,
        device=None,
        use_conv: bool = True,
    ):
        self.state_dim = state_dim
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.device = device or torch.device("cpu")
        self.use_conv = use_conv
        
        # åˆ›å»º Objects
        self.objects = {
            "vision": ObjectNode("vision", obs_dim, device=device),
            "internal": ObjectNode("internal", state_dim, device=device, init="normal"),
            "action": ObjectNode("action", action_dim, device=device),
            "target": ObjectNode("target", action_dim, device=device),
        }
        
        # åˆ›å»ºç”Ÿæˆæ¨¡å‹ Aspects
        self.encoder = EncoderAspect(
            sensory_name="vision",
            internal_name="internal",
            input_dim=obs_dim,
            output_dim=state_dim,
            use_conv=use_conv,
            image_size=28 if use_conv else None,
        ).to(device)
        
        self.observation = ObservationAspect(
            internal_name="internal",
            sensory_name="vision",
            state_dim=state_dim,
            obs_dim=obs_dim,
            use_conv=use_conv,
            image_size=28 if use_conv else None,
        ).to(device)
        
        self.dynamics = DynamicsAspect(
            internal_name="internal",
            action_name="action",
            state_dim=state_dim,
            action_dim=action_dim,
        ).to(device)
        
        self.preference = PreferenceAspect(
            internal_name="internal",
            target_name="target",
            state_dim=state_dim,
            weight=1.0,
        ).to(device)
        
        self.aspects = [
            self.encoder,
            self.observation,
            self.dynamics,
            self.preference,
        ]
        
        # ä¸»åŠ¨æ¨ç†å¾ªç¯
        self.infer_loop = ActiveInferenceLoop(
            objects=self.objects,
            aspects=self.aspects,
            infer_lr=0.01,
            max_grad_norm=10.0,
        )
        
        # ç‹¬ç«‹åˆ†ç±»å™¨ï¼ˆç”¨äºè¯„ä¼°ï¼Œä¸å‚ä¸è‡ªç”±èƒ½è®¡ç®—ï¼‰
        self.classifier = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 10),
        ).to(device)
    
    def compute_free_energy(self):
        """è®¡ç®—æ€»è‡ªç”±èƒ½"""
        return compute_total_free_energy(self.objects, self.aspects)
    
    def sanitize_states(self):
        """æ¸…ç†çŠ¶æ€ï¼ˆé˜²æ­¢ NaN/Infï¼‰"""
        for obj in self.objects.values():
            state = obj.state
            if torch.isnan(state).any() or torch.isinf(state).any():
                obj.state = torch.nan_to_num(state, nan=0.0, posinf=1.0, neginf=-1.0)
            obj.state = torch.clamp(obj.state, -10.0, 10.0)
    
    def predict_class(self, vision_state: torch.Tensor) -> int:
        """é¢„æµ‹ç±»åˆ«"""
        # ä½¿ç”¨ç¼–ç å™¨ç›´æ¥è¾“å‡º
        with torch.no_grad():
            if vision_state.dim() == 1:
                vision_state = vision_state.unsqueeze(0)
            if self.use_conv:
                vision_reshaped = vision_state.view(-1, 1, 28, 28)
            else:
                vision_reshaped = vision_state
            
            internal = self.encoder.encoder(vision_reshaped)
            if internal.dim() > 1:
                internal = internal.squeeze(0)
            
            logits = self.classifier(internal)
            return logits.argmax(dim=-1).item()


def evaluate_accuracy(
    fep_system: PureFEPMNISTClassifier,
    world_interface: MNISTWorldInterface,
    num_samples: int = 1000,
    device=None,
):
    """è¯„ä¼°å‡†ç¡®ç‡"""
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨dropoutç­‰ï¼‰
    for aspect in fep_system.aspects:
        if isinstance(aspect, nn.Module):
            aspect.eval()
    fep_system.classifier.eval()
    
    correct = 0
    
    with torch.no_grad():
        for i in range(num_samples):
            obs = world_interface.reset()
            target = world_interface.get_target()
            true_label = target.argmax().item()
            
            # ä½¿ç”¨ç¼–ç å™¨ç›´æ¥è¾“å‡ºï¼ˆä¸è¿›è¡ŒçŠ¶æ€æ¨ç†ï¼‰
            vision_state = obs["vision"]
            pred_class = fep_system.predict_class(vision_state)
            
            if pred_class == true_label:
                correct += 1
    
    # æ¢å¤è®­ç»ƒæ¨¡å¼
    for aspect in fep_system.aspects:
        if isinstance(aspect, nn.Module):
            aspect.train()
    fep_system.classifier.train()
    
    accuracy = correct / num_samples
    return accuracy


def run_pure_fep_experiment(
    num_steps: int,
    config: Dict,
    device: torch.device,
    *,
    verbose: bool = False,
    output: str = "data/pure_fep_mnist_improved.json",
    save_interval: int = 100,
):
    """è¿è¡Œæ”¹è¿›ç‰ˆçº¯ FEP MNIST å®éªŒ"""
    
    # é…ç½®å‚æ•°
    use_encoder_init = config.get("use_encoder_init", True)  # æ–¹æ¡ˆ2ï¼šä½¿ç”¨ç¼–ç å™¨åˆå§‹åŒ–
    skip_inference = config.get("skip_inference", False)  # æ–¹æ¡ˆ5ï¼šè·³è¿‡çŠ¶æ€æ¨ç†
    num_infer_iters = config.get("num_infer_iters", 2 if use_encoder_init else 5)  # å‡å°‘è¿­ä»£æ¬¡æ•°
    
    # è‡ªç”±èƒ½æƒé‡
    obs_weight = config.get("obs_weight", 0.1)  # é™ä½è§‚å¯Ÿé‡å»ºæƒé‡
    encoder_weight = config.get("encoder_weight", 1.0)  # ä¿æŒç¼–ç å™¨æƒé‡
    pref_weight = config.get("pref_weight", 10.0)  # æé«˜åˆ†ç±»å…ˆéªŒæƒé‡
    
    print("=" * 80)
    print("çº¯ FEP MNIST å®éªŒï¼ˆæ”¹è¿›ç‰ˆï¼‰")
    print("=" * 80)
    print(f"\nğŸ“‹ é…ç½®:")
    print(f"  ä½¿ç”¨ç¼–ç å™¨åˆå§‹åŒ–: {use_encoder_init}")
    print(f"  è·³è¿‡çŠ¶æ€æ¨ç†: {skip_inference}")
    print(f"  çŠ¶æ€æ¨ç†è¿­ä»£æ¬¡æ•°: {num_infer_iters}")
    print(f"  è‡ªç”±èƒ½æƒé‡: F_obs={obs_weight}, F_encoder={encoder_weight}, F_pref={pref_weight}")
    print()
    
    # åˆ›å»ºä¸–ç•Œæ¨¡å‹
    train_world = MNISTWorldModel(
        state_dim=config.get("state_dim", 128),
        action_dim=config.get("action_dim", 10),
        obs_dim=config.get("obs_dim", 784),
        device=device,
        train=True,
    )
    train_interface = MNISTWorldInterface(train_world)
    
    val_world = MNISTWorldModel(
        state_dim=config.get("state_dim", 128),
        action_dim=config.get("action_dim", 10),
        obs_dim=config.get("obs_dim", 784),
        device=device,
        train=False,
    )
    val_interface = MNISTWorldInterface(val_world)
    
    # åˆ›å»º FEP ç³»ç»Ÿ
    fep_system = PureFEPMNISTClassifier(
        state_dim=config.get("state_dim", 128),
        obs_dim=config.get("obs_dim", 784),
        action_dim=config.get("action_dim", 10),
        device=device,
        use_conv=config.get("use_conv", True),
    )
    
    # åˆ›å»ºåˆ†ç¦»ä¼˜åŒ–å™¨ï¼ˆæ–¹æ¡ˆ4ï¼‰
    encoder_optimizer = Adam(
        list(fep_system.encoder.parameters()),
        lr=config.get("encoder_lr", 0.001),
        weight_decay=config.get("weight_decay", 1e-4),
    )
    
    observation_optimizer = Adam(
        list(fep_system.observation.parameters()),
        lr=config.get("observation_lr", 0.0001),  # æ›´ä½çš„å­¦ä¹ ç‡
        weight_decay=config.get("weight_decay", 1e-4),
    )
    
    preference_optimizer = Adam(
        list(fep_system.preference.parameters()),
        lr=config.get("preference_lr", 0.01),  # æ›´é«˜çš„å­¦ä¹ ç‡
        weight_decay=config.get("weight_decay", 1e-4),
    )
    
    classifier_optimizer = Adam(
        list(fep_system.classifier.parameters()),
        lr=config.get("classifier_lr", 0.001),
        weight_decay=config.get("weight_decay", 1e-4),
    )
    
    # å®éªŒè®°å½•
    snapshots = []
    accuracy_history = []
    free_energy_history = []
    F_obs_history = []
    F_dyn_history = []
    F_encoder_history = []
    F_pref_history = []
    
    # åˆå§‹åŒ–è§‚å¯Ÿ
    obs = train_interface.reset()
    prev_state = None
    prev_action = None
    
    progress = tqdm(range(num_steps), desc="Pure FEP MNIST (Improved)")
    
    try:
        for step in progress:
            # 1. è®¾ç½®å½“å‰è§‚å¯Ÿ
            fep_system.objects["vision"].set_state(obs["vision"])
            
            # 2. è®¾ç½®ç›®æ ‡æ ‡ç­¾
            target = train_interface.get_target()
            fep_system.objects["target"].set_state(target)
            
            # 3. çŠ¶æ€æ¨ç†ï¼šæ¨æ–­ internal
            if skip_inference:
                # æ–¹æ¡ˆ5ï¼šç›´æ¥ä½¿ç”¨ç¼–ç å™¨è¾“å‡ºï¼Œè·³è¿‡çŠ¶æ€æ¨ç†
                with torch.no_grad():
                    vision_state = fep_system.objects["vision"].state
                    if vision_state.dim() == 1:
                        vision_state = vision_state.unsqueeze(0)
                    if fep_system.use_conv:
                        vision_reshaped = vision_state.view(-1, 1, 28, 28)
                    else:
                        vision_reshaped = vision_state
                    
                    internal_pred = fep_system.encoder.encoder(vision_reshaped)
                    if internal_pred.dim() > 1:
                        internal_pred = internal_pred.squeeze(0)
                    
                    fep_system.objects["internal"].set_state(internal_pred)
            else:
                # æ–¹æ¡ˆ2ï¼šä½¿ç”¨ç¼–ç å™¨è¾“å‡ºä½œä¸ºåˆå§‹å€¼
                if use_encoder_init:
                    with torch.no_grad():
                        vision_state = fep_system.objects["vision"].state
                        if vision_state.dim() == 1:
                            vision_state = vision_state.unsqueeze(0)
                        if fep_system.use_conv:
                            vision_reshaped = vision_state.view(-1, 1, 28, 28)
                        else:
                            vision_reshaped = vision_state
                        
                        internal_init = fep_system.encoder.encoder(vision_reshaped)
                        if internal_init.dim() > 1:
                            internal_init = internal_init.squeeze(0)
                        
                        # è®¾ç½®ä¸ºéœ€è¦æ¢¯åº¦çš„å¶å­å¼ é‡
                        fep_system.objects["internal"].set_state(
                            internal_init.detach().requires_grad_(True)
                        )
                
                # è¿›è¡Œå°‘é‡è¿­ä»£ä¼˜åŒ–
                fep_system.infer_loop.infer_states(
                    target_objects=("internal",),
                    num_iters=num_infer_iters,
                    sanitize_callback=fep_system.sanitize_states,
                )
            
            current_state = fep_system.objects["internal"].state.clone()
            
            # 4. è¡ŒåŠ¨é€‰æ‹©ï¼ˆç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨åˆ†ç±»å™¨é¢„æµ‹ï¼‰
            with torch.no_grad():
                internal = fep_system.objects["internal"].state
                action_logits = fep_system.classifier(internal)
                action = torch.softmax(action_logits, dim=-1)
                fep_system.objects["action"].set_state(action)
            
            # 5. æ‰§è¡Œè¡ŒåŠ¨ï¼Œè·å–æ–°è§‚å¯Ÿ
            if step > 0:
                obs, reward = train_interface.step(action)
            else:
                obs = train_interface.reset()
            
            # 6. è®¡ç®—è‡ªç”±èƒ½ç»„ä»¶ï¼ˆç”¨äºè®°å½•ï¼‰
            with torch.no_grad():
                F_obs = fep_system.observation.free_energy_contrib(fep_system.objects)
                F_encoder = fep_system.encoder.free_energy_contrib(fep_system.objects)
                
                if prev_state is not None and prev_action is not None:
                    temp_internal_next = ObjectNode("internal_next", fep_system.state_dim, device=device)
                    temp_internal_next.set_state(current_state)
                    temp_objects = fep_system.objects.copy()
                    temp_objects["internal_next"] = temp_internal_next
                    temp_objects["internal"] = ObjectNode("internal", fep_system.state_dim, device=device)
                    temp_objects["internal"].set_state(prev_state)
                    temp_objects["action"] = ObjectNode("action", fep_system.action_dim, device=device)
                    temp_objects["action"].set_state(prev_action)
                    F_dyn = fep_system.dynamics.free_energy_contrib(temp_objects)
                else:
                    F_dyn = torch.tensor(0.0, device=device)
                
                F_pref = fep_system.preference.free_energy_contrib(fep_system.objects)
                F_total = obs_weight * F_obs + encoder_weight * F_encoder + F_dyn + pref_weight * F_pref
                
                F_obs_history.append(F_obs.item())
                F_encoder_history.append(F_encoder.item())
                F_dyn_history.append(F_dyn.item())
                F_pref_history.append(F_pref.item())
                free_energy_history.append(F_total.item())
            
            # 7. å‚æ•°å­¦ä¹ ï¼ˆä½¿ç”¨åˆ†ç¦»ä¼˜åŒ–å™¨ï¼‰
            if step > 0:
                try:
                    # ç¼–ç å™¨å­¦ä¹ 
                    encoder_optimizer.zero_grad()
                    F_encoder = encoder_weight * fep_system.encoder.free_energy_contrib(fep_system.objects)
                    if torch.isfinite(F_encoder) and F_encoder.requires_grad:
                        F_encoder.backward(retain_graph=True)
                        torch.nn.utils.clip_grad_norm_(fep_system.encoder.parameters(), config.get("max_grad_norm", 100.0))
                        encoder_optimizer.step()
                    
                    # è§£ç å™¨å­¦ä¹ 
                    observation_optimizer.zero_grad()
                    F_obs = obs_weight * fep_system.observation.free_energy_contrib(fep_system.objects)
                    if torch.isfinite(F_obs) and F_obs.requires_grad:
                        F_obs.backward(retain_graph=True)
                        torch.nn.utils.clip_grad_norm_(fep_system.observation.parameters(), config.get("max_grad_norm", 100.0))
                        observation_optimizer.step()
                    
                    # å…ˆéªŒå­¦ä¹ 
                    preference_optimizer.zero_grad()
                    F_pref = pref_weight * fep_system.preference.free_energy_contrib(fep_system.objects)
                    if torch.isfinite(F_pref) and F_pref.requires_grad:
                        F_pref.backward(retain_graph=True)
                        torch.nn.utils.clip_grad_norm_(fep_system.preference.parameters(), config.get("max_grad_norm", 100.0))
                        preference_optimizer.step()
                    
                    # åˆ†ç±»å™¨å­¦ä¹ 
                    classifier_optimizer.zero_grad()
                    internal = fep_system.objects["internal"].state
                    logits = fep_system.classifier(internal)
                    target_class = target.argmax().item()
                    F_class = nn.functional.cross_entropy(
                        logits.unsqueeze(0),
                        torch.tensor([target_class], device=device),
                    )
                    if torch.isfinite(F_class) and F_class.requires_grad:
                        F_class.backward()
                        torch.nn.utils.clip_grad_norm_(fep_system.classifier.parameters(), config.get("max_grad_norm", 100.0))
                        classifier_optimizer.step()
                    
                    fep_system.sanitize_states()
                except Exception as e:
                    if verbose:
                        print(f"æ­¥éª¤ {step} å­¦ä¹ é”™è¯¯: {e}")
            
            # 8. è¯„ä¼°å‡†ç¡®ç‡
            if (step + 1) % config.get("eval_interval", 100) == 0:
                acc = evaluate_accuracy(fep_system, train_interface, num_samples=100, device=device)
                accuracy_history.append(acc)
                
                # æ›´æ–°è¿›åº¦æ¡
                F = free_energy_history[-1] if free_energy_history else 0.0
                progress.set_postfix({"F": f"{F:.3f}", "Acc": f"{acc*100:.1f}%"})
            
            # 9. ä¿å­˜å¿«ç…§
            if (step + 1) % save_interval == 0:
                snapshots.append({
                    "step": step + 1,
                    "free_energy": free_energy_history[-1] if free_energy_history else 0.0,
                    "free_energy_obs": F_obs_history[-1] if F_obs_history else 0.0,
                    "free_energy_encoder": F_encoder_history[-1] if F_encoder_history else 0.0,
                    "free_energy_dyn": F_dyn_history[-1] if F_dyn_history else 0.0,
                    "free_energy_pref": F_pref_history[-1] if F_pref_history else 0.0,
                    "accuracy": accuracy_history[-1] if accuracy_history else 0.0,
                })
            
            prev_state = current_state.clone()
            prev_action = action.clone()
    
    except KeyboardInterrupt:
        print("\nå®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nè¯„ä¼°æœ€ç»ˆå‡†ç¡®ç‡...")
    final_acc = evaluate_accuracy(fep_system, train_interface, num_samples=1000, device=device)
    val_acc = evaluate_accuracy(fep_system, val_interface, num_samples=1000, device=device)
    
    # ä¿å­˜æ¨¡å‹æƒé‡
    model_output = output.replace('.json', '_model.pth')
    checkpoint = {
        "config": config,
        "encoder": fep_system.encoder.state_dict(),
        "observation": fep_system.observation.state_dict(),
        "dynamics": fep_system.dynamics.state_dict(),
        "preference": fep_system.preference.state_dict(),
        "classifier": fep_system.classifier.state_dict(),
    }
    torch.save(checkpoint, model_output)
    print(f"âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ°: {model_output}")
    
    # ä¿å­˜ç»“æœ
    results = {
        "config": config,
        "num_steps": num_steps,
        "final_free_energy": free_energy_history[-1] if free_energy_history else 0.0,
        "final_accuracy": final_acc,
        "val_accuracy": val_acc,
        "model_path": model_output,
        "snapshots": snapshots,
        "free_energy_history": free_energy_history,
        "accuracy_history": accuracy_history,
        "F_obs_history": F_obs_history,
        "F_encoder_history": F_encoder_history,
        "F_dyn_history": F_dyn_history,
        "F_pref_history": F_pref_history,
    }
    
    with open(output, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output}")
    print(f"   æœ€ç»ˆè‡ªç”±èƒ½: {results['final_free_energy']:.4f}")
    print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {results['final_accuracy']*100:.2f}%")
    print(f"   éªŒè¯å‡†ç¡®ç‡: {results['val_accuracy']*100:.2f}%")
    
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="çº¯ FEP MNIST å®éªŒï¼ˆæ”¹è¿›ç‰ˆï¼‰")
    parser.add_argument("--steps", type=int, default=1000, help="è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--output", type=str, default="data/pure_fep_mnist_improved.json", help="è¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--save-interval", type=int, default=100, help="ä¿å­˜é—´éš”")
    parser.add_argument("--use-encoder-init", action="store_true", default=True, help="ä½¿ç”¨ç¼–ç å™¨åˆå§‹åŒ–")
    parser.add_argument("--skip-inference", action="store_true", default=False, help="è·³è¿‡çŠ¶æ€æ¨ç†ï¼ˆç›´æ¥ä½¿ç”¨ç¼–ç å™¨è¾“å‡ºï¼‰")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    config = {
        "obs_dim": 784,
        "state_dim": 128,
        "action_dim": 10,
        "use_conv": True,
        "infer_lr": 0.01,
        "learning_rate": 0.001,
        "encoder_lr": 0.001,
        "observation_lr": 0.0001,  # æ›´ä½çš„å­¦ä¹ ç‡
        "preference_lr": 0.01,  # æ›´é«˜çš„å­¦ä¹ ç‡
        "classifier_lr": 0.001,
        "weight_decay": 1e-4,
        "classification_weight": 1.0,
        "num_infer_iters": 2,  # å‡å°‘è¿­ä»£æ¬¡æ•°
        "eval_infer_iters": 1,
        "num_action_iters": 3,
        "action_lr": 0.1,
        "max_grad_norm": 100.0,
        "eval_interval": 100,
        # æ”¹è¿›å‚æ•°
        "use_encoder_init": args.use_encoder_init,
        "skip_inference": args.skip_inference,
        "obs_weight": 0.1,  # é™ä½è§‚å¯Ÿé‡å»ºæƒé‡
        "encoder_weight": 1.0,  # ä¿æŒç¼–ç å™¨æƒé‡
        "pref_weight": 10.0,  # æé«˜åˆ†ç±»å…ˆéªŒæƒé‡
    }
    
    run_pure_fep_experiment(
        num_steps=args.steps,
        config=config,
        device=device,
        verbose=args.verbose,
        output=args.output,
        save_interval=args.save_interval,
    )

