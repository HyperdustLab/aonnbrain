# 卷积编码器/解码器实验结果分析

## 实验概述

对比了线性编码器/解码器和卷积编码器/解码器在 MNIST 主动推理任务中的表现。

## 实验结果对比

### 最终性能

| 指标 | 线性网络 | 卷积网络 | 变化 |
|------|---------|---------|------|
| 最终自由能 | 172.22 | 273.72 | +101.50 |
| 训练准确率 | 15.0% | 8.0% | -7.0% |
| 验证准确率 | 9.8% | 9.4% | -0.4% |
| F_obs (最终) | 166.73 | 269.52 | +102.79 |
| F_obs 占比 | 96.8% | 98.5% | +1.7% |

### 学习过程

| 指标 | 线性网络 | 卷积网络 |
|------|---------|---------|
| 初始自由能 | 234.25 | 565.51 |
| 自由能降低幅度 | 26.48% | 51.60% |
| F_obs 降低幅度 | 约 7% | 52.1% |

## 关键发现

### 1. 初始状态差异

- **卷积网络的初始自由能更高** (565.51 vs 234.25)
  - 原因：卷积网络参数更多（625K vs ~100K），随机初始化导致更大的初始预测误差
  - 影响：需要更多训练步数才能收敛

### 2. F_obs 占主导

- **卷积网络的 F_obs 更高** (269.52 vs 166.73)
  - 原因：卷积解码器从 128 维生成 28x28 图像，任务更复杂
  - 影响：F_obs 占主导（98.5%），是主要的学习瓶颈

### 3. 学习速度

- **卷积网络的自由能降低幅度更大** (51.60% vs 26.48%)
  - 说明：卷积网络在学习，但起点更高
  - 问题：1000 步可能不足以让卷积网络充分学习

### 4. 准确率下降

- **卷积网络的准确率更低** (8.0% vs 15.0%)
  - 原因：
    1. 卷积网络需要更多训练步数
    2. F_obs 太高，影响状态推理质量
    3. 可能学习率不适合卷积网络

## 问题诊断

### 问题 1: 为什么卷积网络表现不如线性网络？

**根本原因**：
1. **参数更多**：卷积网络有 625K 参数，线性网络只有 ~100K
2. **初始化问题**：随机初始化导致初始自由能更高
3. **训练不足**：1000 步可能不足以让卷积网络充分学习
4. **学习率可能不合适**：卷积网络可能需要更小的学习率

### 问题 2: 为什么 F_obs 更高？

**原因**：
- 卷积解码器需要学习从 128 维状态生成 28x28 图像
- 这是一个更复杂的生成任务
- 需要学习空间结构和像素级别的细节

**影响**：
- F_obs 占主导（98.5%）
- 影响整体自由能
- 可能影响状态推理质量

### 问题 3: 为什么准确率更低？

**原因**：
1. **F_obs 太高**：影响状态推理，导致 internal 状态质量下降
2. **训练不足**：卷积网络需要更多训练步数
3. **学习率问题**：可能需要调整学习率

## 改进建议

### 1. 降低学习率

**当前**：`learning_rate = 0.001`

**建议**：`learning_rate = 0.0001` 或 `0.0005`

**理由**：卷积网络参数更多，需要更小的学习率来稳定训练

### 2. 增加训练步数

**当前**：1000 步

**建议**：5000+ 步

**理由**：卷积网络需要更多时间学习复杂的生成模型

### 3. 改进初始化

**当前**：默认 PyTorch 初始化

**建议**：使用 Xavier/Kaiming 初始化

**理由**：更好的初始化可以降低初始自由能，加速收敛

### 4. 调整自由能权重

**当前**：F_obs 权重 = 1.0

**建议**：
- 降低 F_obs 权重：0.5 或 0.7
- 提高 F_class 权重：1.5 或 2.0

**理由**：平衡不同组件的学习速度，避免 F_obs 占主导

### 5. 使用预训练权重

**当前**：随机初始化

**建议**：使用预训练的 VAE 权重初始化

**理由**：加速收敛，降低初始自由能

### 6. 添加 Batch Normalization

**建议**：在卷积层之间添加 BatchNorm

**理由**：稳定训练，加速收敛

### 7. 使用更深的网络

**当前**：3 层卷积 + 3 层转置卷积

**建议**：增加网络深度或使用残差连接

**理由**：提高生成模型的能力

## 预期效果（如果应用改进）

如果应用所有改进（降低学习率、增加训练步数、改进初始化等）：

- **初始自由能**：可能降低 30-50%
- **最终自由能**：可能降低到 100-150
- **F_obs**：可能降低到 100-150
- **准确率**：可能提升到 20-30%
- **需要训练步数**：5000+ 步

## 结论

1. **卷积网络在学习**：自由能降低幅度更大（51.60% vs 26.48%）
2. **但需要更多训练**：1000 步不足以让卷积网络充分学习
3. **需要调整超参数**：学习率、初始化等需要优化
4. **潜力很大**：如果正确训练，卷积网络应该能超越线性网络

## 下一步

1. 应用改进建议（降低学习率、增加训练步数）
2. 运行 5000+ 步的长训练实验
3. 对比改进后的结果
4. 如果仍然不理想，考虑使用预训练权重或更复杂的架构

