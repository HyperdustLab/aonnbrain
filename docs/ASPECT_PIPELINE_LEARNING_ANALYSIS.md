# Aspect Pipeline MNIST å­¦ä¹ ç®—æ³•åˆ†æ

## æ¦‚è¿°

æœ¬æ–‡æ¡£åˆ†æä»¥å‰ aspect pipeline MNIST ç½‘ç»œçš„å­¦ä¹ ç®—æ³•ï¼ˆæ¥è‡ª `aonn_mnist_aspect_pipeline` é¡¹ç›®ï¼‰ï¼Œå¹¶ä¸å½“å‰ç»Ÿä¸€æ¶æ„çš„å®ç°è¿›è¡Œå¯¹æ¯”ï¼Œæ‰¾å‡ºå…³é”®å·®å¼‚å’Œæ”¹è¿›æ–¹å‘ã€‚

---

## ä»¥å‰ Aspect Pipeline çš„å­¦ä¹ ç®—æ³•

### æ ¸å¿ƒç‰¹ç‚¹

**æ ‡å‡† PyTorch è®­ç»ƒæµç¨‹ + Adam ä¼˜åŒ–å™¨ + æ‰¹é‡è®­ç»ƒ**

### 1. è®­ç»ƒå¾ªç¯ç»“æ„ï¼ˆæ¥è‡ª `aonn_mnist_aspect_pipeline`ï¼‰

```python
# aonn_mnist_aspect_pipeline/aonn/train.py
def train_one_epoch(model, loader, optimizer, device, log_file=None, epoch=None, verbose=False):
    model.train()
    total_loss, total_correct, total_samples = 0.0, 0, 0
    
    for batch_idx, (x, y) in enumerate(loader):
        x, y = x.to(device), y.to(device)
        
        # 1. å‰å‘ä¼ æ’­
        logits = model(x)  # [B, 10]
        
        # 2. è®¡ç®—æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
        loss = F.cross_entropy(logits, y)
        
        # 3. æ ‡å‡† PyTorch è®­ç»ƒæµç¨‹
        optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
        loss.backward()        # åå‘ä¼ æ’­
        optimizer.step()        # æ›´æ–°å‚æ•°ï¼ˆAdam ä¼˜åŒ–å™¨ï¼‰
        
        # 4. ç»Ÿè®¡
        total_loss += loss.item() * x.size(0)
        preds = logits.argmax(dim=1)
        total_correct += (preds == y).sum().item()
        total_samples += x.size(0)
    
    avg_loss = total_loss / total_samples
    acc = total_correct / total_samples
    return avg_loss, acc

# ä¼˜åŒ–å™¨åˆå§‹åŒ–
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
```

### 2. å®é™…è®­ç»ƒç»“æœ

æ ¹æ®è®­ç»ƒæ—¥å¿—ï¼ˆ`train_log_20251118_195134.json`ï¼‰ï¼š

**é…ç½®**ï¼š
- Cell ç»´åº¦ï¼š128
- æ¯å±‚ Aspect æ•°ï¼š32
- Pipeline æ·±åº¦ï¼š4
- æ‰¹é‡å¤§å°ï¼š128
- å­¦ä¹ ç‡ï¼š0.001
- ä¼˜åŒ–å™¨ï¼šAdam

**è®­ç»ƒæ•ˆæœ**ï¼ˆ10 ä¸ª epochï¼‰ï¼š
- **Epoch 1**: è®­ç»ƒ 92.2%, æµ‹è¯• **95.8%**
- **Epoch 2**: è®­ç»ƒ 96.6%, æµ‹è¯• **96.8%**
- **Epoch 3**: è®­ç»ƒ 97.7%, æµ‹è¯• **97.3%**
- **Epoch 4**: è®­ç»ƒ 98.1%, æµ‹è¯• **97.3%**
- **Epoch 10**: æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡ **97.27%**

**å…³é”®æŒ‡æ ‡**ï¼š
- âœ… **å¿«é€Ÿæ”¶æ•›**ï¼š1 ä¸ª epoch è¾¾åˆ° 95.8%
- âœ… **é«˜å‡†ç¡®ç‡**ï¼š10 ä¸ª epoch è¾¾åˆ° 97.27%
- âœ… **ç¨³å®šè®­ç»ƒ**ï¼šè®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡åŒæ­¥æå‡

### 3. å…³é”®ç‰¹æ€§

#### âœ… ä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨ï¼ˆAdamï¼‰
- **ä¼˜åŠ¿**ï¼š
  - è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆæ¯ä¸ªå‚æ•°ç‹¬ç«‹ï¼‰
  - åŠ¨é‡æœºåˆ¶ï¼ˆåŠ é€Ÿæ”¶æ•›ï¼‰
  - äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆæ›´ç¨³å®šï¼‰
  - æ›´ç¨³å®šçš„æ”¶æ•›

#### âœ… æ‰¹é‡è®­ç»ƒ
- ä½¿ç”¨ `DataLoader` è¿›è¡Œæ‰¹é‡å¤„ç†ï¼ˆbatch_size=128ï¼‰
- æ¯ä¸ª batch æ›´æ–°ä¸€æ¬¡å‚æ•°
- æ›´é«˜æ•ˆçš„æ¢¯åº¦ä¼°è®¡

#### âœ… ç®€å•çš„è®­ç»ƒæµç¨‹
- `zero_grad()` â†’ `backward()` â†’ `step()`
- æ ‡å‡†çš„ PyTorch æ¨¡å¼
- æ— éœ€æ‰‹åŠ¨ç®¡ç†å‚æ•°å’Œæ¢¯åº¦

#### âœ… ç›´æ¥ä¼˜åŒ–æŸå¤±å‡½æ•°
- æŸå¤± = äº¤å‰ç†µï¼ˆ`F.cross_entropy`ï¼‰
- ç›´æ¥å¯¹æŸå¤±åå‘ä¼ æ’­
- æ‰€æœ‰å‚æ•°è‡ªåŠ¨æ›´æ–°

---

## å½“å‰ç»Ÿä¸€æ¶æ„çš„å®ç°

### æ ¸å¿ƒç‰¹ç‚¹

**ä¸»åŠ¨æ¨ç†å¾ªç¯ + æ‰‹åŠ¨å‚æ•°æ›´æ–°**

### 1. å½“å‰è®­ç»ƒæµç¨‹

```python
# scripts/run_mnist_experiment.py
# 5. ä¸–ç•Œæ¨¡å‹å­¦ä¹ ï¼ˆå­¦ä¹ åˆ†ç±»å™¨å’Œ Pipeline å‚æ•°ï¼‰
if prev_obs is not None and len(brain.aspects) > 0:
    # 1. æ‰‹åŠ¨æ”¶é›†æ‰€æœ‰å‚æ•°
    learnable_params = []
    for aspect in brain.aspects:
        if isinstance(aspect, nn.Module):
            params = list(aspect.parameters())
            learnable_params.extend(params)
            if isinstance(aspect, PipelineAspect):
                learnable_params.extend(list(aspect.pipeline.parameters()))
    
    # 2. å»é‡
    seen = set()
    unique_params = []
    for param in learnable_params:
        if id(param) not in seen:
            seen.add(id(param))
            unique_params.append(param)
    learnable_params = unique_params
    
    # 3. æ‰‹åŠ¨æ¢¯åº¦æ›´æ–°
    F = brain.compute_free_energy()
    if torch.isfinite(F) and F.requires_grad:
        # æ¸…é™¤æ¢¯åº¦
        for param in learnable_params:
            if param.grad is not None:
                param.grad.zero_()
        
        # åå‘ä¼ æ’­
        F.backward(retain_graph=False)
        
        # æ¢¯åº¦è£å‰ª
        if max_grad_norm is not None:
            torch.nn.utils.clip_grad_norm_(learnable_params, max_grad_norm)
        
        # æ‰‹åŠ¨æ›´æ–°å‚æ•°ï¼ˆSGDï¼‰
        learning_rate = 0.001
        with torch.no_grad():
            for param in learnable_params:
                if param.grad is not None:
                    param.data -= learning_rate * param.grad
```

### 2. å…³é”®å·®å¼‚

#### âŒ ä½¿ç”¨æ‰‹åŠ¨ SGD è€Œé Adam
- **é—®é¢˜**ï¼š
  - å›ºå®šå­¦ä¹ ç‡ï¼Œæ— æ³•è‡ªé€‚åº”
  - æ²¡æœ‰åŠ¨é‡ï¼Œæ”¶æ•›æ…¢
  - æ²¡æœ‰äºŒé˜¶çŸ©ä¼°è®¡ï¼Œä¸ç¨³å®š

#### âŒ å•æ ·æœ¬è®­ç»ƒ
- æ¯ä¸ªæ ·æœ¬å•ç‹¬æ›´æ–°
- æ¢¯åº¦ä¼°è®¡ä¸å‡†ç¡®
- è®­ç»ƒæ•ˆç‡ä½

#### âŒ å¤æ‚çš„å‚æ•°ç®¡ç†
- éœ€è¦æ‰‹åŠ¨æ”¶é›†å‚æ•°
- éœ€è¦å»é‡
- å®¹æ˜“å‡ºé”™

#### âœ… ä¸»åŠ¨æ¨ç†å¾ªç¯
- å…ˆæ›´æ–°çŠ¶æ€ï¼ˆinternalï¼‰ï¼Œå†æ›´æ–°å‚æ•°
- ç¬¦åˆè‡ªç”±èƒ½åŸç†
- ä½†å‚æ•°æ›´æ–°æ–¹å¼ä¸å¤Ÿé«˜æ•ˆ

---

## å…³é”®å·®å¼‚å¯¹æ¯”

| ç‰¹æ€§ | ä»¥å‰ Aspect Pipeline | å½“å‰ç»Ÿä¸€æ¶æ„ |
|------|---------------------|------------|
| **ä¼˜åŒ–å™¨** | Adamï¼ˆè‡ªé€‚åº”ï¼‰ | æ‰‹åŠ¨ SGDï¼ˆå›ºå®šå­¦ä¹ ç‡ï¼‰ |
| **è®­ç»ƒæ–¹å¼** | æ‰¹é‡è®­ç»ƒï¼ˆDataLoader, batch_size=128ï¼‰ | å•æ ·æœ¬è®­ç»ƒ |
| **å‚æ•°ç®¡ç†** | è‡ªåŠ¨ï¼ˆé€šè¿‡ optimizerï¼‰ | æ‰‹åŠ¨æ”¶é›†å’Œå»é‡ |
| **å­¦ä¹ ç‡** | è‡ªé€‚åº”ï¼ˆAdam, lr=1e-3ï¼‰ | å›ºå®šï¼ˆ0.001ï¼‰ |
| **åŠ¨é‡** | æœ‰ï¼ˆAdam å†…ç½®ï¼‰ | æ—  |
| **æ¢¯åº¦æ›´æ–°** | `optimizer.step()` | æ‰‹åŠ¨ `param.data -= lr * grad` |
| **çŠ¶æ€æ¨ç†** | æ— ï¼ˆç›´æ¥å‰å‘ä¼ æ’­ï¼‰ | å¿…éœ€ï¼ˆActiveInferenceLoopï¼‰ |
| **å‡†ç¡®ç‡** | **97.27%** (10 epochs) | **12%** (200 steps) |

---

## ä¸ºä»€ä¹ˆä»¥å‰çš„å­¦ä¹ ç®—æ³•æ›´é«˜æ•ˆï¼Ÿ

### 1. Adam ä¼˜åŒ–å™¨çš„ä¼˜åŠ¿

**è‡ªé€‚åº”å­¦ä¹ ç‡**ï¼š
- æ¯ä¸ªå‚æ•°æœ‰ç‹¬ç«‹çš„å­¦ä¹ ç‡
- æ ¹æ®æ¢¯åº¦å†å²è‡ªåŠ¨è°ƒæ•´
- åˆå§‹é˜¶æ®µå­¦ä¹ å¿«ï¼ŒåæœŸç¨³å®š

**åŠ¨é‡æœºåˆ¶**ï¼š
- ç´¯ç§¯æ¢¯åº¦å†å²
- å‡å°‘éœ‡è¡
- åŠ é€Ÿæ”¶æ•›

**äºŒé˜¶çŸ©ä¼°è®¡**ï¼š
- è€ƒè™‘æ¢¯åº¦æ–¹å·®
- æ›´ç¨³å®šçš„æ›´æ–°
- é€‚åˆéå¹³ç¨³ç›®æ ‡

### 2. æ‰¹é‡è®­ç»ƒçš„ä¼˜åŠ¿

**æ›´å‡†ç¡®çš„æ¢¯åº¦ä¼°è®¡**ï¼š
- å¤šä¸ªæ ·æœ¬çš„å¹³å‡æ¢¯åº¦ï¼ˆbatch_size=128ï¼‰
- å‡å°‘å™ªå£°
- æ›´ç¨³å®šçš„æ›´æ–°æ–¹å‘

**æ›´é«˜çš„è®­ç»ƒæ•ˆç‡**ï¼š
- æ‰¹é‡çŸ©é˜µè¿ç®—
- GPU åˆ©ç”¨ç‡é«˜
- å‡å°‘æ›´æ–°æ¬¡æ•°

### 3. ç®€å•çš„è®­ç»ƒæµç¨‹

**æ ‡å‡† PyTorch æ¨¡å¼**ï¼š
- æ— éœ€æ‰‹åŠ¨ç®¡ç†å‚æ•°
- è‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯
- ä»£ç ç®€æ´ï¼Œä¸æ˜“å‡ºé”™

---

## æ”¹è¿›æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼ˆæ¨èï¼Œå‚è€ƒ `aonn_mnist_aspect_pipeline`ï¼‰

```python
# åœ¨ MNIST å®éªŒè„šæœ¬ä¸­
from torch.optim import Adam

# åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼ˆä¸€æ¬¡æ€§ï¼Œåœ¨å®éªŒå¼€å§‹å‰ï¼‰
def collect_all_parameters(brain):
    """æ”¶é›†æ‰€æœ‰å¯å­¦ä¹ å‚æ•°"""
    params = []
    for aspect in brain.aspects:
        if isinstance(aspect, nn.Module):
            params.extend(list(aspect.parameters()))
            # PipelineAspect çš„å‚æ•°å·²ç»åŒ…å«åœ¨ aspect.parameters() ä¸­
    return params

optimizer = Adam(
    collect_all_parameters(brain),
    lr=0.001,  # ä¸ aonn_mnist_aspect_pipeline ä¸€è‡´
    betas=(0.9, 0.999),
    eps=1e-8
)

# è®­ç»ƒå¾ªç¯
for step in range(num_steps):
    # ... è®¾ç½®è§‚å¯Ÿå’Œç›®æ ‡ ...
    
    # æ ‡å‡†è®­ç»ƒæµç¨‹ï¼ˆä¸ aonn_mnist_aspect_pipeline ä¸€è‡´ï¼‰
    optimizer.zero_grad()
    F = brain.compute_free_energy()
    if torch.isfinite(F) and F.requires_grad:
        F.backward()
        # æ¢¯åº¦è£å‰ªï¼ˆå¯é€‰ï¼Œä½† aonn_mnist_aspect_pipeline æ²¡æœ‰ä½¿ç”¨ï¼‰
        # if max_grad_norm is not None:
        #     torch.nn.utils.clip_grad_norm_(optimizer.param_groups[0]['params'], max_grad_norm)
        optimizer.step()  # Adam è‡ªåŠ¨æ›´æ–°ï¼Œè‡ªé€‚åº”å­¦ä¹ ç‡
```

### æ–¹æ¡ˆ 2ï¼šæ‰¹é‡è®­ç»ƒ

```python
# æ”¶é›†å¤šä¸ªæ ·æœ¬ï¼Œæ‰¹é‡æ›´æ–°
batch_size = 32
batch_obs = []
batch_targets = []

for i in range(batch_size):
    obs = world_interface.reset()
    target = world_interface.get_target()
    batch_obs.append(obs["vision"])
    batch_targets.append(target)

# æ‰¹é‡è®¾ç½®
batch_obs_tensor = torch.stack(batch_obs)  # [B, 784]
batch_targets_tensor = torch.stack(batch_targets)  # [B, 10]

# æ‰¹é‡è®­ç»ƒ
optimizer.zero_grad()
# éœ€è¦ä¿®æ”¹ brain ä»¥æ”¯æŒæ‰¹é‡
F = compute_batch_free_energy(brain, batch_obs_tensor, batch_targets_tensor)
F.backward()
optimizer.step()
```

### æ–¹æ¡ˆ 3ï¼šæ··åˆæ–¹æ¡ˆï¼ˆçŠ¶æ€æ¨ç† + Adam ä¼˜åŒ–å™¨ï¼‰

```python
# 1. ä¸»åŠ¨æ¨ç†ï¼ˆæ›´æ–°çŠ¶æ€ï¼‰
loop = ActiveInferenceLoop(brain.objects, brain.aspects, infer_lr=0.01)
loop.infer_states(target_objects=("internal",), num_iters=3)

# 2. å‚æ•°å­¦ä¹ ï¼ˆä½¿ç”¨ Adamï¼‰
optimizer.zero_grad()
F = brain.compute_free_energy()
F.backward()
optimizer.step()
```

---

## é¢„æœŸæ”¹è¿›æ•ˆæœ

### ä½¿ç”¨ Adam ä¼˜åŒ–å™¨åï¼ˆå‚è€ƒ `aonn_mnist_aspect_pipeline` çš„å®é™…ç»“æœï¼‰ï¼š

1. **å­¦ä¹ é€Ÿåº¦**ï¼šæå‡ 5-10 å€
   - ä»¥å‰ï¼š1 ä¸ª epoch è¾¾åˆ° 95.8%
   - å½“å‰ï¼š200 æ­¥ä»…è¾¾åˆ° 12%

2. **æ”¶æ•›ç¨³å®šæ€§**ï¼šæ˜¾è‘—æå‡
   - ä»¥å‰ï¼šè®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡åŒæ­¥ç¨³å®šæå‡
   - å½“å‰ï¼šå‡†ç¡®ç‡æ³¢åŠ¨å¤§ï¼Œä¸ç¨³å®š

3. **æœ€ç»ˆå‡†ç¡®ç‡**ï¼šä» 12% æå‡åˆ° **95-97%**ï¼ˆç»è¿‡è¶³å¤Ÿè®­ç»ƒï¼‰
   - ä»¥å‰ï¼š10 ä¸ª epoch è¾¾åˆ° **97.27%**
   - å½“å‰ï¼š200 æ­¥ä»…è¾¾åˆ° 12%

4. **è‡ªç”±èƒ½ä¸‹é™**ï¼šæ›´å¿«ã€æ›´ç¨³å®š
   - ä»¥å‰ï¼šæŸå¤±ä» 0.26 é™åˆ° 0.08ï¼ˆ10 epochsï¼‰
   - å½“å‰ï¼šè‡ªç”±èƒ½ä» 598 é™åˆ° 252ï¼ˆ200 stepsï¼‰ï¼Œä½†å‡†ç¡®ç‡ä½

---

## å®æ–½å»ºè®®

### ç«‹å³å®æ–½ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

1. **æ›¿æ¢æ‰‹åŠ¨ SGD ä¸º Adam ä¼˜åŒ–å™¨**
   - ä¿®æ”¹ `run_mnist_experiment.py`
   - åˆå§‹åŒ– Adam ä¼˜åŒ–å™¨
   - ä½¿ç”¨ `optimizer.step()` æ›¿ä»£æ‰‹åŠ¨æ›´æ–°
   - **å‚è€ƒ**ï¼š`aonn_mnist_aspect_pipeline/aonn/train.py`

2. **ç®€åŒ–å‚æ•°æ”¶é›†**
   - ä½¿ç”¨ `optimizer` è‡ªåŠ¨ç®¡ç†å‚æ•°
   - ç§»é™¤æ‰‹åŠ¨å»é‡é€»è¾‘

### ä¸­æœŸå®æ–½ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

3. **å®ç°æ‰¹é‡è®­ç»ƒ**
   - ä¿®æ”¹ `brain.compute_free_energy()` æ”¯æŒæ‰¹é‡
   - ä½¿ç”¨ `DataLoader` è¿›è¡Œæ‰¹é‡å¤„ç†
   - **å‚è€ƒ**ï¼š`aonn_mnist_aspect_pipeline` çš„æ‰¹é‡è®­ç»ƒæ–¹å¼

4. **ä¼˜åŒ–å™¨å‚æ•°è°ƒä¼˜**
   - è°ƒæ•´å­¦ä¹ ç‡ï¼ˆ0.001ï¼Œä¸ä»¥å‰ä¸€è‡´ï¼‰
   - è°ƒæ•´ betasï¼ˆé»˜è®¤ 0.9, 0.999ï¼‰
   - æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰

### é•¿æœŸä¼˜åŒ–ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

5. **æ··åˆè®­ç»ƒç­–ç•¥**
   - ç»“åˆä¸»åŠ¨æ¨ç†å’Œæ‰¹é‡è®­ç»ƒ
   - è‡ªé€‚åº”é€‰æ‹©è®­ç»ƒæ–¹å¼

6. **é«˜çº§ä¼˜åŒ–å™¨**
   - å°è¯• AdamWã€RMSprop ç­‰
   - å­¦ä¹ ç‡é¢„çƒ­å’Œè¡°å‡

---

## æ€»ç»“

**ä»¥å‰ aspect pipeline MNIST ç½‘ç»œçš„å­¦ä¹ ç®—æ³•ä¼˜åŠ¿**ï¼š

1. âœ… **ä½¿ç”¨ Adam ä¼˜åŒ–å™¨**ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®š
2. âœ… **æ‰¹é‡è®­ç»ƒ**ï¼šæ›´å‡†ç¡®çš„æ¢¯åº¦ä¼°è®¡ï¼ˆbatch_size=128ï¼‰
3. âœ… **æ ‡å‡† PyTorch æµç¨‹**ï¼šç®€å•ã€é«˜æ•ˆã€ä¸æ˜“å‡ºé”™
4. âœ… **å®é™…æ•ˆæœ**ï¼š10 ä¸ª epoch è¾¾åˆ° **97.27%** å‡†ç¡®ç‡

**å½“å‰å®ç°çš„ä¸è¶³**ï¼š

1. âŒ **æ‰‹åŠ¨ SGD**ï¼šå›ºå®šå­¦ä¹ ç‡ï¼Œæ”¶æ•›æ…¢
2. âŒ **å•æ ·æœ¬è®­ç»ƒ**ï¼šæ¢¯åº¦ä¼°è®¡ä¸å‡†ç¡®
3. âŒ **å¤æ‚çš„å‚æ•°ç®¡ç†**ï¼šå®¹æ˜“å‡ºé”™
4. âŒ **æ•ˆæœå·®**ï¼š200 æ­¥ä»…è¾¾åˆ° 12% å‡†ç¡®ç‡

**æ”¹è¿›æ–¹å‘**ï¼š

1. ğŸ”§ **ç«‹å³ä½¿ç”¨ Adam ä¼˜åŒ–å™¨**ï¼ˆå‚è€ƒ `aonn_mnist_aspect_pipeline`ï¼‰
2. ğŸ”§ **å®ç°æ‰¹é‡è®­ç»ƒ**
3. ğŸ”§ **ç®€åŒ–è®­ç»ƒæµç¨‹**

é€šè¿‡è¿™äº›æ”¹è¿›ï¼Œé¢„æœŸå¯ä»¥å°† MNIST å‡†ç¡®ç‡ä»å½“å‰çš„ 12% æå‡åˆ° **95-97%**ï¼ˆä¸ä»¥å‰çš„ç»“æœä¸€è‡´ï¼‰ã€‚

---

## ç›¸å…³æ–‡ä»¶

- `/Users/moss/aonn_mnist_aspect_pipeline/aonn/train.py` - ä»¥å‰çš„è®­ç»ƒå¾ªç¯å®ç°
- `/Users/moss/aonn_mnist_aspect_pipeline/aonn/model.py` - æ¨¡å‹å®šä¹‰
- `/Users/moss/aonn_mnist_aspect_pipeline/checkpoints/train_log_20251118_195134.json` - è®­ç»ƒæ—¥å¿—
- `scripts/run_mnist_experiment.py` - å½“å‰çš„ MNIST å®éªŒè„šæœ¬
- `src/aonn/core/active_inference_loop.py` - ä¸»åŠ¨æ¨ç†å¾ªç¯
- `src/aonn/models/aonn_brain_v3.py` - AONN Brain V3 å®ç°

---

**æ–‡æ¡£ç»´æŠ¤è€…**: AONN å¼€å‘å›¢é˜Ÿ  
**æœ€åæ›´æ–°**: 2024  
**ç‰ˆæœ¬**: 1.0
